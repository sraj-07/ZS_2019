# -*- coding: utf-8 -*-
"""ZS Data Science Challenge

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11ABR3UuyfD8TWo8JLqYIT0r1oHDv8Hp2
"""

import pandas as pd
import matplotlib.pyplot as plt

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

file_list = drive.ListFile({'q': "'1X8XdX2zTw9vugvFHdLkNoECXFZwRAMr0' in parents and trashed=false"}).GetList()
for file1 in file_list:
  print('title: %s, id: %s' % (file1['title'], file1['id']))

data = drive.CreateFile({'id': '1OKObCRcOH1z7DfnEcb2SVdw5pceg4fct'})
data.GetContentFile('data.csv')

import pandas as pd
import numpy as np
data = pd.read_csv('data.csv')

data.head()

data.isnull().sum()

data.shape

train = data[data.is_goal.notnull()]
test = data[data.is_goal.isnull()]

train.drop('Unnamed: 0',axis=1,inplace=True)
test.drop('Unnamed: 0',axis=1,inplace=True)

rem_min = train[['remaining_min','remaining_min.1']]

rem_min.reset_index(inplace=True)

rem_min.head()

ad = []
for i in range(0,rem_min.shape[0]):
  if rem_min['remaining_min'][i] == rem_min['remaining_min.1'][i]:
    ad.append(True)
  else:
    ad.append(False)

data['new'] = data['remaining_min'].isnull()

data['remaining_min.1'] = data.apply(lambda row: np.nan if (row['remaining_min.1']>90) else row['remaining_min.1'],axis=1)

data['remaining_min'] = data.apply(lambda row: row['remaining_min.1'] if np.isnan(row['remaining_min']) else row['remaining_min'],axis=1)

data['remaining_sec.1'] = data.apply(lambda row: np.nan if (row['remaining_sec.1']>60) else row['remaining_sec.1'],axis=1)
data['remaining_sec'] = data.apply(lambda row: row['remaining_sec.1'] if np.isnan(row['remaining_sec']) else row['remaining_sec'],axis=1)

data['distance_of_shot'] = data.apply(lambda row: row['distance_of_shot.1'] if np.isnan(row['distance_of_shot']) else row['distance_of_shot'],axis=1)

data['power_of_shot'] = data.apply(lambda row: row['power_of_shot.1'] if np.isnan(row['power_of_shot']) else row['power_of_shot'],axis=1)

data['knockout_match'] = data.apply(lambda row: row['knockout_match.1'] if np.isnan(row['knockout_match']) else row['knockout_match'],axis=1)

ref = data[['match_id','home/away','lat/lng']]

ref = ref.dropna(subset=['home/away','lat/lng'])

ref = ref.groupby(['match_id','home/away','lat/lng']).sum()

ref.reset_index(inplace=True)

train_temp = data.drop(['home/away', 'lat/lng'],axis=1)

train_temp = train_temp.merge(ref,on='match_id',how="outer")

train_temp['home'] = train_temp.apply(lambda row: str(row['home/away'])[:4] if '@' in row['home/away'] else str(row['home/away'][9:]),axis=1)

train_temp['away'] = train_temp.apply(lambda row: str(row['home/away'])[7:] if '@' in row['home/away'] else str(row['home/away'][:4]),axis=1)

ref_1 = train_temp[['match_id','game_season']]

ref_1 = ref_1.dropna(subset=['match_id','game_season'])

ref_1 = ref_1.groupby(['match_id','game_season']).sum()

ref_1.reset_index(inplace=True)

train_temp_1 = train_temp.drop(['game_season'],axis=1)

train_temp_1 = train_temp_1.merge(ref_1,on='match_id',how="outer")

file = train_temp_1[['shot_id_number','location_x','location_y','remaining_min','remaining_sec','power_of_shot','knockout_match','game_season','distance_of_shot','type_of_shot','type_of_combined_shot','shot_basics','range_of_shot','home','away','is_goal']]

file.head()

from sklearn import preprocessing

le = preprocessing.LabelEncoder()

cols = ['game_season','shot_basics','home','away','range_of_shot','type_of_shot','type_of_combined_shot']

is_goal = file['is_goal']

file.fillna('-999',inplace=True)

file['range_of_shot'].value_counts()

for i in cols:
  print(i)
  file[i] = le.fit_transform(file[i])

file.replace('-999',999,inplace=True)

file['is_goal'] = is_goal

import xgboost
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

file.columns

file['total_time_rem'] = file['remaining_sec'] + file['remaining_min']*60

train = file[file.is_goal.notnull()]
test = file[file.is_goal.isnull()]

train.columns

X_train = train[['location_x', 'location_y', 'remaining_min', 'remaining_sec',
       'power_of_shot', 'knockout_match', 'game_season', 'distance_of_shot','type_of_shot', 'type_of_combined_shot',
       'shot_basics', 'range_of_shot', 'home', 'away','total_time_rem']]

y_train = train['is_goal']

clf = XGBClassifier(learning_rate = 0.05,n_estimators=100,random_state = 42)

parameters = {
        'n_estimators': [10,50,100,360,450],
        'num_boost_round': [10, 15,25, 50],
        'eta': [0.05, 0.1, 0.3],
        'max_depth': [3, 4, 5],
        'subsample': [0.9, 1.0],
        'colsample_bytree': [0.9, 1.0],
    }

clf = GridSearchCV(clf, parameters, n_jobs=1, cv=2)

!pip install --user --upgrade scikit-learn

model = clf.fit(X_train, y_train)

best_parameters, score, _ = max(clf.cv_results_, key=lambda x: x[1])
print('Raw AUC score:', score)
for param_name in sorted(best_parameters.keys()):
  print("%s: %r" % (param_name, best_parameters[param_name]))

pred = model.predict_proba(test[['location_x', 'location_y', 'remaining_min', 'remaining_sec',
       'power_of_shot', 'knockout_match', 'game_season', 'distance_of_shot','type_of_shot', 'type_of_combined_shot',
       'shot_basics', 'range_of_shot', 'home', 'away','total_time_rem']])

test['is_goal'] = pred

subm = test[['shot_id_number','is_goal']]

subm.to_csv('submission_5.csv',index=False)
file = drive.CreateFile({'parents':[{u'id': '1X8XdX2zTw9vugvFHdLkNoECXFZwRAMr0'}]})
file.SetContentFile("submission_5.csv")
file.Upload()

model.feature_importances_

